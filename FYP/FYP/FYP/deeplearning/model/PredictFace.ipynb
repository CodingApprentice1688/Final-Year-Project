{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290554e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read() #read one frame\n",
    "cap.release()\n",
    "cv2.imshow('image', frame)\n",
    "cv2.imwrite(\"framed.jpg\", frame)  \n",
    "if cv2.waitKey(0) & 0xff == ord('q'): # press q to exit\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a5d2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " flatten_291 (Flatten)       (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1016 (Dense)          (None, 6)                 12294     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,600,006\n",
      "Trainable params: 12,294\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('my_modelv3.h5')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328e7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate face detection on 5 Celebrity Faces Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPool2D, Dropout\n",
    "from numpy import asarray\n",
    "from matplotlib import pyplot\n",
    "from mtcnn import MTCNN\n",
    "from os import listdir\n",
    "\n",
    "def extract_face(filename, required_size=(200, 200)):\n",
    "    # load image from file\n",
    "    image = Image.open(filename)\n",
    "    image = image.convert('RGB')\n",
    "    pixels = asarray(image)\n",
    "    # use MTCNN face detector to detect faces inside the image\n",
    "    detector = MTCNN()\n",
    "    results = detector.detect_faces(pixels)\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    # bug fix\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bfded12",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Desktop/framed.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pixels \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../Desktop/framed.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m hello \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m pyplot\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mextract_face\u001b[1;34m(filename, required_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_face\u001b[39m(filename, required_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m)):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# load image from file\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m     pixels \u001b[38;5;241m=\u001b[39m asarray(image)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 2953\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2954\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Desktop/framed.jpg'"
     ]
    }
   ],
   "source": [
    "pixels = extract_face('../Desktop/framed.jpg')\n",
    "hello = []\n",
    "pyplot.subplot(2, 7, 1)\n",
    "pyplot.axis('off')\n",
    "pyplot.imshow(pixels)\n",
    "pyplot.show()\n",
    "pixels = pixels.astype('float32')\n",
    "pixels = np.array(pixels)\n",
    "hello.append(pixels)\n",
    "hello = np.array(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2da317b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction shape: [[0.2885074  0.46903995 0.03692678 0.17537315 0.00256144 0.02759128]]\n"
     ]
    }
   ],
   "source": [
    "yhat=model.predict(hello)\n",
    "print(\"prediction shape:\", yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65ad6cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3768858039.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    for subdir, dirs, files in os.walk(\"C:\\Users\\leong\\source\\repos\\csci321_fyp\\FYP\\FYP\\FYP\\deeplearning\\val\"):\u001b[0m\n\u001b[1;37m                                                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_test = []\n",
    "i = 0\n",
    "for subdir, dirs, files in os.walk(\"../Desktop/celeb/val\"):\n",
    "    if i == 0:\n",
    "        y_test = dirs\n",
    "    i = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3999937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person is: jonathandoe\n"
     ]
    }
   ],
   "source": [
    "prediction_index = np.argmax(yhat, axis=None, out=None)\n",
    "prediction = y_test[prediction_index]\n",
    "\n",
    "print(\"The person is: \" + prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd028dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
